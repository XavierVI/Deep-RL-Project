### Reproducing results
Both python scripts can be ran using `python <reinforce/sarsa>_lunar.py [--parallel] [--num_processes=<int>]`. I wanted to get extra credit for this assignment, so when `--parallel` is specified, all 18 hyperparameter configurations will be ran using a process pool of size `--num_processes` (default number is 2, 8 worked well for me).
- The graphs in the report can be reproduced by running each script with `--parallel`, which will generate the data for each hyperparameter config and save the rewards to `results` as a CSV file.
- The plots can be generated by simply running `python plot.py`. These plots are also saved in the `results` directory.

The results directory contains the figures used in the report and the output generated while training the agents. The output includes the rewards from multiple agents being trained in parallel.


### Global Config and Training
The entries

```
"learning_rate": 0.001,
"gamma": 0.99,
```

are only used when running the python scripts without `--parallel`. If `--parallel` is given, it will try to run all possible combinations of 

```
"learning_rates": [0.0001, 0.001, 0.01],
"gammas": [0.90, 0.95, 0.99],
"boltzmann_options": [true, false]
```

using a process pool, so multiple agents can be trained at the same time.

I added additional parameters to `global_config.json` because I wanted to fine tune epsilon-greedy and Boltzmann sampling as much as possible to find the best one for each algorithm.


### Reinforce
Action selection was rewritten to only use python torch functions. This slightly improved performance. I also added the computation of the log probabilities to action selection so the code is a bit more clean, and we only make one inference per time step.

Boltzmann action selection was redone to be more numerically stable and correct. This time, the policy network produces both the logits and the softmax probabilities, and Boltzmann sampling uses the logits and temperature to create the distribution to sample from. Interestingly, when computing the log probabilities from the Boltzmann distribution, REINFORCE can actually get rewards as high as 150.


### Plotting
A moving average with a window size of `N` can actually be computed using a convolution between the rewards and an array of ones divided by the window size (5).


### Max Timesteps
Setting the maximum number of time steps to 1000 did lead to really high rewards, but this would slow down training significantly. 500 seems to be yielding some decent results.


### References
1. [Lunar Lander Docs](https://gymnasium.farama.org/environments/box2d/lunar_lander/)
2. [Wikipedia: Moving Average]()
3. [NumPy: ufunc.at (used for computing averages in plot.py)](https://numpy.org/devdocs//reference/generated/numpy.ufunc.at.html)
4. [NumPy: Convolve](https://numpy.org/doc/stable/reference/generated/numpy.convolve.html)
5. [Water Programming: Implementation of the Moving Average Filter Using Convolution](https://waterprogramming.wpcomstaging.com/2018/09/04/implementation-of-the-moving-average-filter-using-convolution/)