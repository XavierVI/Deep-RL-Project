{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedc83a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43918aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaQNet(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim, hidden, act):\n",
    "        super().__init__()\n",
    "\n",
    "        act_fn = nn.ReLU()\n",
    "        self.layers = nn.ModuleList()\n",
    "        prev = s_dim\n",
    "\n",
    "        for h in hidden:\n",
    "            self.layers.append(nn.Linear(prev, h))\n",
    "            prev = h\n",
    "\n",
    "        self.out = nn.Linear(prev, a_dim)\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        for l in self.layers:\n",
    "            x = self.act_fn(l(x))\n",
    "\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b98cc2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8])\n",
      "tensor([[-0.0341, -1.7673,  1.6315, -1.2016,  1.2342, -0.5946,  0.9670, -0.5552],\n",
      "        [ 0.5648, -0.1728,  0.5526,  0.6931,  0.6063, -0.2072,  0.0939, -0.6567],\n",
      "        [ 1.4145, -0.8998,  0.3079,  0.6522, -0.9508, -0.0288, -0.0244, -1.3830],\n",
      "        [-0.5737, -1.5074,  1.3948, -0.2741, -0.2908,  0.4586, -0.9738,  0.5688]])\n",
      "torch.Size([4, 4])\n",
      "tensor([[-0.1469, -0.0085, -0.0791,  0.0235],\n",
      "        [ 0.0102,  0.0943,  0.0214, -0.0465],\n",
      "        [ 0.1680,  0.1256, -0.0166,  0.0761],\n",
      "        [-0.0286, -0.0851, -0.0384,  0.0174]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generating Q values for multiple states\n",
    "s_dim = 8\n",
    "states = torch.randn(4, s_dim)  # Batch of 4 states\n",
    "print(states.shape)\n",
    "print(states)\n",
    "\n",
    "q_net = SarsaQNet(s_dim=s_dim, a_dim=4, hidden=[64, 64], act='relu')\n",
    "q_values = q_net(states)\n",
    "print(q_values.shape)  # Should print torch.Size([4, 4])\n",
    "print(q_values)  # Should print a tensor of shape (4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71be1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE: Extracting Q(s,a) for specific actions\n",
      "============================================================\n",
      "\n",
      "q_values shape: torch.Size([3, 4])\n",
      "q_values:\n",
      "tensor([[ 0.5000,  1.2000, -0.3000,  0.8000],\n",
      "        [ 0.1000,  0.7000,  1.5000,  0.2000],\n",
      "        [ 0.9000,  0.3000,  0.6000,  1.1000]])\n",
      "\n",
      "actions shape: torch.Size([3])\n",
      "actions: tensor([1, 2, 3])\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 1: actions.unsqueeze(1)\n",
      "------------------------------------------------------------\n",
      "Shape: torch.Size([3, 1])\n",
      "Values:\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 2: q_values.gather(1, actions.unsqueeze(1))\n",
      "------------------------------------------------------------\n",
      "Shape: torch.Size([3, 1])\n",
      "Values:\n",
      "tensor([[1.2000],\n",
      "        [1.5000],\n",
      "        [1.1000]])\n",
      "\n",
      "What happened:\n",
      "  Experience 0: q_values[0, 1] = 1.2000000476837158\n",
      "  Experience 1: q_values[1, 2] = 1.5\n",
      "  Experience 2: q_values[2, 3] = 1.100000023841858\n",
      "\n",
      "------------------------------------------------------------\n",
      "STEP 3: .squeeze(1)\n",
      "------------------------------------------------------------\n",
      "Shape: torch.Size([3])\n",
      "Values: tensor([1.2000, 1.5000, 1.1000])\n",
      "\n",
      "============================================================\n",
      "COMPLETE OPERATION (one line):\n",
      "============================================================\n",
      "pred_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
      "Result: tensor([1.2000, 1.5000, 1.1000])\n",
      "Shape: torch.Size([3])\n",
      "\n",
      "============================================================\n",
      "VERIFICATION: Compare with manual indexing\n",
      "============================================================\n",
      "Manual indexing: tensor([1.2000, 1.5000, 1.1000])\n",
      "Gather method:   tensor([1.2000, 1.5000, 1.1000])\n",
      "Are they equal? True\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example: How gather() works for extracting Q-values\n",
    "print(\"=\"*60)\n",
    "print(\"EXAMPLE: Extracting Q(s,a) for specific actions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Simulate 3 experiences with 4 possible actions each\n",
    "batch_size = 3\n",
    "action_dim = 4\n",
    "\n",
    "# Q-values for all actions (output from Q-network)\n",
    "q_values = torch.tensor([\n",
    "    [0.5, 1.2, -0.3, 0.8],  # Experience 0: Q-values for actions [0,1,2,3]\n",
    "    [0.1, 0.7,  1.5, 0.2],  # Experience 1: Q-values for actions [0,1,2,3]\n",
    "    [0.9, 0.3,  0.6, 1.1]   # Experience 2: Q-values for actions [0,1,2,3]\n",
    "])\n",
    "\n",
    "# Actions actually taken in each experience\n",
    "actions = torch.tensor([1, 2, 3])  # Took action 1, 2, and 3 respectively\n",
    "\n",
    "print(f\"\\nq_values shape: {q_values.shape}\")\n",
    "print(f\"q_values:\\n{q_values}\\n\")\n",
    "\n",
    "print(f\"actions shape: {actions.shape}\")\n",
    "print(f\"actions: {actions}\\n\")\n",
    "\n",
    "# Step 1: unsqueeze to add dimension\n",
    "print(\"-\" * 60)\n",
    "print(\"STEP 1: actions.unsqueeze(1)\")\n",
    "print(\"-\" * 60)\n",
    "actions_unsqueezed = actions.unsqueeze(1)\n",
    "print(f\"Shape: {actions_unsqueezed.shape}\")\n",
    "print(f\"Values:\\n{actions_unsqueezed}\\n\")\n",
    "\n",
    "# Step 2: gather to extract specific Q-values\n",
    "print(\"-\" * 60)\n",
    "print(\"STEP 2: q_values.gather(1, actions.unsqueeze(1))\")\n",
    "print(\"-\" * 60)\n",
    "gathered = q_values.gather(1, actions_unsqueezed)\n",
    "print(f\"Shape: {gathered.shape}\")\n",
    "print(f\"Values:\\n{gathered}\")\n",
    "print(\"\\nWhat happened:\")\n",
    "print(f\"  Experience 0: q_values[0, {actions[0]}] = {q_values[0, actions[0]]}\")\n",
    "print(f\"  Experience 1: q_values[1, {actions[1]}] = {q_values[1, actions[1]]}\")\n",
    "print(f\"  Experience 2: q_values[2, {actions[2]}] = {q_values[2, actions[2]]}\\n\")\n",
    "\n",
    "# Step 3: squeeze to remove extra dimension\n",
    "print(\"-\" * 60)\n",
    "print(\"STEP 3: .squeeze(1)\")\n",
    "print(\"-\" * 60)\n",
    "pred_q_values = gathered.squeeze(1)\n",
    "print(f\"Shape: {pred_q_values.shape}\")\n",
    "print(f\"Values: {pred_q_values}\\n\")\n",
    "\n",
    "# Complete operation in one line\n",
    "print(\"=\"*60)\n",
    "print(\"COMPLETE OPERATION (one line):\")\n",
    "print(\"=\"*60)\n",
    "result = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "print(f\"pred_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\")\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Shape: {result.shape}\\n\")\n",
    "\n",
    "# Verify it matches manual indexing\n",
    "print(\"=\"*60)\n",
    "print(\"VERIFICATION: Compare with manual indexing\")\n",
    "print(\"=\"*60)\n",
    "manual_result = torch.tensor([\n",
    "    q_values[0, actions[0]],\n",
    "    q_values[1, actions[1]],\n",
    "    q_values[2, actions[2]]\n",
    "])\n",
    "print(f\"Manual indexing: {manual_result}\")\n",
    "print(f\"Gather method:   {result}\")\n",
    "print(f\"Are they equal? {torch.equal(manual_result, result)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
