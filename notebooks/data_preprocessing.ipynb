{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f5e5d2",
   "metadata": {},
   "source": [
    "This notebook is used to preprocess the CocoDoom dataset to allow for faster training.\n",
    "\n",
    "Each image will be loaded, preprocessed, and saved as a tensor shard in the same location as the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15647086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project directory to path for imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.pardir))\n",
    "\n",
    "from PIL import Image\n",
    "from Vision.datasets import CocoDoomDataset\n",
    "from transformers import DetrImageProcessor\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a54d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.95s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n",
      "index created!\n",
      "Loaded run-train.json\n",
      "Number of images: 50732\n",
      "Number of Categories: 94\n"
     ]
    }
   ],
   "source": [
    "# create preprocessor\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# create dataset instance\n",
    "dataset = CocoDoomDataset(\n",
    "    data_dir=os.path.join(os.pardir, os.pardir, \"datasets\", \"cocodoom\"),\n",
    "    annotation_file_name=\"run-train.json\",\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f7fe12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing train: 100%|██████████| 1000/1000 [00:16<00:00, 60.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: saved 1, skipped 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# full preprocessing was moved to a python script in Vision\n",
    "from tqdm import tqdm\n",
    "\n",
    "split_name = \"train\"\n",
    "saved, skipped = 0, 0\n",
    "save_root = os.path.join(\n",
    "    os.pardir, os.pardir, \"datasets\", \"cocodoom\", \"preprocessed\"\n",
    ")\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(1000), desc=f\"Preprocessing {split_name}\"):\n",
    "    image, target, img_file_name = dataset.get_image(i)\n",
    "\n",
    "    encoding = processor(\n",
    "        images=image,\n",
    "        annotations=target,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    pixel_values = encoding['pixel_values'].squeeze()\n",
    "    target = dict(encoding['labels'][0])\n",
    "\n",
    "    # reduce format of target tensors\n",
    "    target['boxes'] = target['boxes'].to(torch.float16)\n",
    "    del target['size']\n",
    "    del target['orig_size']\n",
    "    # we only have 94 categories\n",
    "    target['class_labels'] = target['class_labels'].to(torch.int16)\n",
    "    del target['area']  # remove area to save space\n",
    "    del target['iscrowd'] # remove iscrowd to save space\n",
    "\n",
    "    # modify file name to have .pt extension\n",
    "    pt_file_name = os.path.splitext(img_file_name)[0] + \".pt\"\n",
    "    save_path = os.path.join(save_root, pt_file_name)\n",
    "\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    torch.save(\n",
    "        {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": target\n",
    "        },\n",
    "        save_path\n",
    "    )\n",
    "    saved += 1\n",
    "\n",
    "print(f\"{split_name}: saved {saved}, skipped {skipped}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "894ef904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pixel values shape: torch.Size([3, 800, 1280])\n",
      "Loaded labels: {'size': tensor([ 800, 1280]), 'image_id': tensor([1010000002]), 'class_labels': tensor([0, 0]), 'boxes': tensor([[0.4328, 0.6225, 0.0531, 0.1250],\n",
      "        [0.5484, 0.5700, 0.0469, 0.1000]]), 'area': tensor([2880., 1840.]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([200, 320])}\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "pixel_values, labels = dataset[0]\n",
    "print(f\"Loaded pixel values shape: {pixel_values.shape}\")\n",
    "print(f\"Loaded labels: {labels}\")\n",
    "\n",
    "print(f\"{labels['class_labels'].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a0c358f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CocoDoomDataset' object has no attribute 'slow__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m start_time = time.perf_counter()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     pixel_values, labels = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslow__getitem__\u001b[49m(i)\n\u001b[32m      7\u001b[39m end_time = time.perf_counter()\n\u001b[32m      8\u001b[39m elapsed_time = end_time - start_time\n",
      "\u001b[31mAttributeError\u001b[39m: 'CocoDoomDataset' object has no attribute 'slow__getitem__'"
     ]
    }
   ],
   "source": [
    "# benchmarking __getitem__ when including preprocessing\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for i in range(1000):\n",
    "    pixel_values, labels = dataset.slow__getitem__(i)\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to fetch 1000 items: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a130c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to fetch 1000 items: 0.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# benchmarking __getitem__ without preprocessing\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "for i in range(1000):\n",
    "    pixel_values, labels = dataset[i]\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to fetch 1000 items: {elapsed_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doom-venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
