{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b9cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815ebf6",
   "metadata": {},
   "source": [
    "## Gymnasium API Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c64e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=None)\n",
    "\n",
    "env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9cfbe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2 = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "env2.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1c35d",
   "metadata": {},
   "source": [
    "#### Action space dimension for continuous parallel environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8efb8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs = gym.make_vec(\"Pendulum-v1\", num_envs=2, vectorization_mode=\"sync\")\n",
    "\n",
    "envs.single_action_space.shape[0]\n",
    "envs.action_space.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0210c",
   "metadata": {},
   "source": [
    "#### Action space dimension for discrete parallel environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3dc0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs = gym.make_vec(\"LunarLander-v3\", num_envs=2, vectorization_mode=\"sync\")\n",
    "\n",
    "envs.single_action_space.n\n",
    "envs.action_space.shape # gives number of parallel environments\n",
    "envs.action_space[0].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603203f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d13221f",
   "metadata": {},
   "source": [
    "## Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3caf4d",
   "metadata": {},
   "source": [
    "for A2C, we have several tensors with the following shapes:\n",
    "- states: (N + 1,)\n",
    "- actions: (N, )\n",
    "- rewards: (N, )\n",
    "- log_probs: (N, )\n",
    "- dones: (N, )\n",
    "where N is the number of rewards received.\n",
    "\n",
    "Initially, these were saved to their own separate lists. But, this forces the API to be sequential and inefficient. To introduce parallelism, we need to generalize this by using Tensors and NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c691849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1]\n",
      "Actions:\n",
      "3\n",
      "1\n",
      "Observations:\n",
      "[ 0.00465546  1.4247642   0.24004106  0.29480776 -0.00680472 -0.08300382\n",
      "  0.          0.        ]\n",
      "[-0.00712929  1.3984202  -0.3651231  -0.29073122  0.0096392   0.11075787\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Lunar Lander uses 4 dimensional discrete action space\n",
    "envs = gym.make_vec(\"LunarLander-v3\", num_envs=2, vectorization_mode=\"sync\")\n",
    "envs.reset(seed=42)\n",
    "\n",
    "actions = envs.action_space.sample()\n",
    "\n",
    "obs, rewards, terminates, truncates, infos = envs.step(actions)\n",
    "\n",
    "print(actions)\n",
    "\n",
    "print(\"Actions:\")\n",
    "for a in actions:\n",
    "    print(a)\n",
    "\n",
    "print(\"Observations:\")\n",
    "for ob in obs:\n",
    "    print(ob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94e013",
   "metadata": {},
   "source": [
    "To parallelize our environment, we can actually create the following numpy arrays instead of lists:\n",
    "- states: (T + 1, state_space_size, E)\n",
    "- rewards: (T, E)\n",
    "- log_probs: (T, E)\n",
    "where T is the number of timesteps and E is the number of environments. This actually means we can keep our original code, and parallelize our operations over each environment. This seems pretty efficient and is the best we can do for these algorithms, which have data dependencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-rl (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
