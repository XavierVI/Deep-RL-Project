{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b9cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d815ebf6",
   "metadata": {},
   "source": [
    "## Gymnasium API Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61c64e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=None)\n",
    "\n",
    "env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9cfbe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2 = gym.make(\"LunarLander-v3\", render_mode=None)\n",
    "\n",
    "env2.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e1c35d",
   "metadata": {},
   "source": [
    "#### Action space dimension for continuous parallel environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8efb8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs = gym.make_vec(\"Pendulum-v1\", num_envs=2, vectorization_mode=\"sync\")\n",
    "\n",
    "envs.single_action_space.shape[0]\n",
    "envs.action_space.shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0210c",
   "metadata": {},
   "source": [
    "#### Action space dimension for discrete parallel environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3dc0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "envs = gym.make_vec(\"LunarLander-v3\", num_envs=2, vectorization_mode=\"sync\")\n",
    "\n",
    "envs.single_action_space.n\n",
    "envs.action_space.shape # gives number of parallel environments\n",
    "envs.action_space[0].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603203f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d13221f",
   "metadata": {},
   "source": [
    "## Tensor Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3caf4d",
   "metadata": {},
   "source": [
    "for A2C, we have several tensors with the following shapes:\n",
    "- states: (N + 1,)\n",
    "- actions: (N, )\n",
    "- rewards: (N, )\n",
    "- log_probs: (N, )\n",
    "- dones: (N, )\n",
    "where N is the number of rewards received.\n",
    "\n",
    "Initially, these were saved to their own separate lists. But, this forces the API to be sequential and inefficient. To introduce parallelism, we need to generalize this by using Tensors and NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c691849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1]\n",
      "Actions:\n",
      "3\n",
      "1\n",
      "Observations:\n",
      "[ 0.00465546  1.4247642   0.24004106  0.29480776 -0.00680472 -0.08300382\n",
      "  0.          0.        ]\n",
      "[-0.00712929  1.3984202  -0.3651231  -0.29073122  0.0096392   0.11075787\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Lunar Lander uses 4 dimensional discrete action space\n",
    "envs = gym.make_vec(\"LunarLander-v3\", num_envs=2, vectorization_mode=\"sync\")\n",
    "envs.reset(seed=42)\n",
    "\n",
    "actions = envs.action_space.sample()\n",
    "\n",
    "obs, rewards, terminates, truncates, infos = envs.step(actions)\n",
    "\n",
    "print(actions)\n",
    "\n",
    "print(\"Actions:\")\n",
    "for a in actions:\n",
    "    print(a)\n",
    "\n",
    "print(\"Observations:\")\n",
    "for ob in obs:\n",
    "    print(ob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a94e013",
   "metadata": {},
   "source": [
    "To parallelize our environment, we can actually create the following numpy arrays instead of lists:\n",
    "- states: (T + 1, state_space_size, E)\n",
    "- rewards: (T, E)\n",
    "- log_probs: (T, E)\n",
    "where T is the number of timesteps and E is the number of environments. This actually means we can keep our original code, and parallelize our operations over each environment. This seems pretty efficient and is the best we can do for these algorithms, which have data dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852f9f8b",
   "metadata": {},
   "source": [
    "# A2C Mini Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c23b6d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "*\n",
      "* Using config file: /home/xavier/projects/BipedalWalker/config/a2c_config.json\n",
      "* Using device: cuda\n",
      "*\n",
      "==================================================\n",
      "Using activation function: relu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 2.7455471e-03,  1.2283065e-05, -1.6016494e-03, -1.6000077e-02,\n",
       "          9.2544638e-02,  3.7181317e-03,  8.5972905e-01, -1.5536519e-03,\n",
       "          1.0000000e+00,  3.2846723e-02,  3.7179857e-03,  8.5350478e-01,\n",
       "         -2.5298817e-03,  1.0000000e+00,  4.4081339e-01,  4.4581950e-01,\n",
       "          4.6142212e-01,  4.8954949e-01,  5.3410202e-01,  6.0246021e-01,\n",
       "          7.0914787e-01,  8.8593054e-01,  1.0000000e+00,  1.0000000e+00],\n",
       "        [ 2.7474896e-03, -6.9800271e-06,  5.4296060e-04, -1.5999954e-02,\n",
       "          9.2052542e-02, -7.1652001e-04,  8.6021698e-01,  1.9326812e-03,\n",
       "          1.0000000e+00,  3.2458264e-02, -7.1647146e-04,  8.5376561e-01,\n",
       "          5.0673110e-04,  1.0000000e+00,  4.4081402e-01,  4.4582012e-01,\n",
       "          4.6142277e-01,  4.8955020e-01,  5.3410280e-01,  6.0246104e-01,\n",
       "          7.0914888e-01,  8.8593185e-01,  1.0000000e+00,  1.0000000e+00]],\n",
       "       dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add project root to sys.path\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "here = Path(os.getcwd()).resolve()\n",
    "sys.path.append(str(here.parent))\n",
    "from networks import *\n",
    "from agents import *\n",
    "\n",
    "import json\n",
    "\n",
    "config_file_name = \"a2c_config.json\"\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "#                     Loading JSON Config File                    #\n",
    "# --------------------------------------------------------------- #\n",
    "script_dir = os.path.dirname(os.path.abspath(here))\n",
    "config_path = os.path.join(script_dir, \"config\", config_file_name)\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"=\" * 50)\n",
    "print(\"*\")\n",
    "print(f\"* Using config file: {config_path}\")\n",
    "print(f\"* Using device: {device}\")\n",
    "print(\"*\")\n",
    "print(\"=\" * 50)\n",
    "# add device to cfg\n",
    "cfg[\"device\"] = device\n",
    "np.random.seed(cfg[\"seed\"])\n",
    "torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "agent = A2CAgent(cfg)\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "envs = gym.make_vec(\"BipedalWalker-v3\", num_envs=2, vectorization_mode=\"sync\")\n",
    "env.reset()\n",
    "envs.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "da7b5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45927367  0.6635488   0.04158337  0.896212  ]\n",
      "[[-0.45927367  0.6635488   0.04158337  0.896212  ]\n",
      " [-0.45927367  0.6635488   0.04158337  0.896212  ]]\n",
      "==================================================\n",
      "[ 0.0275713   0.02042601  0.02294076  0.02646169 -0.30862036 -0.9203621\n",
      "  1.7221355   1.0037428   1.         -0.40061867 -0.91190886  1.7239717\n",
      "  0.9924399   1.          0.45596296  0.4611411   0.47727996  0.506374\n",
      "  0.5524577   0.62316513  0.7335194   0.9163776   1.          1.        ]\n",
      "[[ 0.02757297  0.02045241  0.02138251  0.02655961 -0.3080452  -0.9191171\n",
      "   1.7215612   1.0037447   1.         -0.39995864 -0.91069496  1.7232398\n",
      "   0.99246436  1.          0.45599863  0.46117717  0.4773173   0.5064136\n",
      "   0.5525009   0.6232138   0.7335768   0.9164493   1.          1.        ]\n",
      " [ 0.02757056  0.02041726  0.0234743   0.02643744 -0.3087883  -0.92078876\n",
      "   1.7222743   1.00374     1.         -0.4008174  -0.9123346   1.7241647\n",
      "   0.99244195  1.          0.4559534   0.46113142  0.47726995  0.50636333\n",
      "   0.55244607  0.623152    0.733504    0.9163584   1.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# sample random actions from single environment's action space\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "actions = np.array([action for _ in range(envs.num_envs)])\n",
    "print(actions)\n",
    "print(\"=\" * 50)\n",
    "# use step() to generate next states from random actions\n",
    "obs, rewards, terminates, truncates, infos = env.step(action)\n",
    "print(obs)\n",
    "obss, rewards, terminates, truncates, infos = envs.step(actions)\n",
    "print(obss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86580dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5041, 0.4959]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Action: tensor([0], device='cuda:0')\n",
      "Log Probability: tensor([-0.6850], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Entropy: tensor([0.6931], device='cuda:0', grad_fn=<NegBackward0>)\n",
      "==================================================\n",
      "torch.Size([4, 4])\n",
      "tensor([[0.5044, 0.4956],\n",
      "        [0.5038, 0.4962],\n",
      "        [0.5039, 0.4961],\n",
      "        [0.5036, 0.4964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Actions: [0 0 0 0]\n",
      "Log Probabilities: tensor([-0.6844, -0.6855, -0.6854, -0.6859], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Entropies: tensor([0.6931, 0.6931, 0.6931, 0.6931], device='cuda:0',\n",
      "       grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# compute output of actor network for single state\n",
    "# If the environment is not vectorized, we need to add a batch dimension\n",
    "# using unsqueeze(0)\n",
    "state_t = torch.FloatTensor(obs).unsqueeze(0).to(agent.device)\n",
    "state_t.shape\n",
    "logits_t, action_probs = agent.actor(state_t)\n",
    "print(action_probs)\n",
    "\n",
    "dist = torch.distributions.Categorical(probs=action_probs)\n",
    "action = dist.sample()\n",
    "log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "print(\"Action:\", action)\n",
    "print(\"Log Probability:\", log_prob)\n",
    "print(\"Entropy:\", entropy)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "# for vectorized envs\n",
    "states_t = torch.FloatTensor(obss).to(agent.device)\n",
    "print(states_t.shape)\n",
    "logits_t, actions_probs = agent.actor(states_t)\n",
    "print(actions_probs)\n",
    "# In both cases, we just need to use this code\n",
    "dist = torch.distributions.Categorical(probs=actions_probs)\n",
    "actions = dist.sample()\n",
    "log_probs = dist.log_prob(actions).sum(dim=-1)\n",
    "entropies = dist.entropy().sum(dim=-1)\n",
    "\n",
    "print(\"Actions:\", actions.cpu().numpy())\n",
    "print(\"Log Probabilities:\", log_probs)\n",
    "print(\"Entropies:\", entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9eae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1041,  0.0455,  0.1117, -0.1002]], device='cuda:0',\n",
      "       grad_fn=<ClampBackward1>)\n",
      "Action: tensor([-0.6209,  1.0000,  0.6564, -0.9662], device='cuda:0')\n",
      "Log Probability: tensor(-5.0351, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Entropy: tensor(5.8368, device='cuda:0', grad_fn=<SumBackward1>)\n",
      "==================================================\n",
      "torch.Size([2, 24])\n",
      "tensor([[0.5044, 0.4956],\n",
      "        [0.5038, 0.4962],\n",
      "        [0.5039, 0.4961],\n",
      "        [0.5036, 0.4964]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Actions: [[-0.819049   -1.         -1.         -1.        ]\n",
      " [-0.5251849   0.9991143   0.69724464 -0.10119598]]\n",
      "Log Probabilities: tensor([-5.3571, -4.6022], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "Entropies: tensor([5.8367, 5.8368], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# continuous action spaces\n",
    "# If the action space is multi-dimensional, we need to use the sum\n",
    "# across the actions to generate a single log probability and entropy value\n",
    "state_t = torch.FloatTensor(obs).unsqueeze(0).to(agent.device)\n",
    "state_t.shape\n",
    "logits_t, action_probs = agent.actor(state_t)\n",
    "print(action_probs)\n",
    "mean, log_std = agent.actor(state_t)\n",
    "mean = mean.squeeze()\n",
    "log_std = log_std.squeeze()\n",
    "std = torch.exp(log_std)\n",
    "\n",
    "dist = torch.distributions.Normal(mean, std)\n",
    "action_t = dist.sample()\n",
    "action_t = torch.clamp(action_t, -1, 1)\n",
    "log_prob_t = dist.log_prob(action_t).sum(dim=-1)\n",
    "entropy_t = dist.entropy().sum(dim=-1)\n",
    "\n",
    "print(\"Action:\", action_t)\n",
    "print(\"Log Probability:\", log_prob_t)\n",
    "print(\"Entropy:\", entropy_t)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "# for vectorized envs\n",
    "states_t = torch.FloatTensor(obss).to(agent.device)\n",
    "print(states_t.shape)\n",
    "means, log_stds = agent.actor(states_t)\n",
    "print(actions_probs)\n",
    "\n",
    "dist = torch.distributions.Normal(means, torch.exp(log_stds))\n",
    "action_t = dist.sample()\n",
    "action_t = torch.clamp(action_t, -1, 1)\n",
    "log_prob_t = dist.log_prob(action_t).sum(dim=-1)\n",
    "entropy_t = dist.entropy().sum(dim=-1)\n",
    "\n",
    "print(\"Actions:\", action_t.cpu().numpy())\n",
    "print(\"Log Probabilities:\", log_prob_t)\n",
    "print(\"Entropies:\", entropy_t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-rl (3.14.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
